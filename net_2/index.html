<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Impact of GPU Networking (Part 2) | Scale Ops </title> <meta name="author" content=" "> <meta name="description" content="Part 2 builds on earlier experiments by examining how distributing 4 GPUs across 1, 2, and 4 nodes impacts transformer model training, with a focus on network topology and NIC sharing."> <meta name="keywords" content="HPC, AI, ML, gpus, scaling, llms, transformers, parallelism, profiling, performance"> <link rel="stylesheet" href="/scale-ops/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/scale-ops/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/scale-ops/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/scale-ops/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/scale-ops/assets/img/favicon.png?cd856673a519774152b6b16a4efad7c4"> <link rel="stylesheet" href="/scale-ops/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://tomaslaz.github.io/scale-ops/net_2/"> <script src="/scale-ops/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/scale-ops/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/scale-ops/assets/js/distillpub/template.v2.js"></script> <script src="/scale-ops/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "The Impact of GPU Networking (Part 2)",
            "description": "Part 2 builds on earlier experiments by examining how distributing 4 GPUs across 1, 2, and 4 nodes impacts transformer model training, with a focus on network topology and NIC sharing.",
            "published": "March 14, 2025",
            "authors": [
              
              {
                "author": "Tomas",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <script>
    function goToTop() {
      document.body.scrollTop = 0; // For Safari
      document.documentElement.scrollTop = 0; // For Chrome, Firefox, IE and Opera
    }

    // When the user scrolls down 20px from the top of the document, show the button
    window.onscroll = function() {scrollFunction()};

    function scrollFunction() {
      // Get the button:
      let mybutton = document.getElementById("top-button");

      if (document.body.scrollTop > 40 || document.documentElement.scrollTop > 40) {
        mybutton.style.display = "block";
      } else {
        mybutton.style.display = "none";
      }
  }
  </script> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/scale-ops/"> Scale Ops </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="left-button section-button"><a href="../net_1"><svg viewbox="-78.5 0 512 512"><path d="M257 64L291 98 128 262 291 426 257 460 61 262 257 64Z"></path></svg></a></div> <div class="right-button section-button"><a href=""><svg viewbox="-78.5 0 512 512"><path d="M98 460L64 426 227 262 64 98 98 64 294 262 98 460Z"></path></svg></a></div> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/scale-ops/"> </a> </li> <li class="nav-item nav-hidden"><a class="nav-link" onclick="goToTop()" id="top-button" style="display: none;">Back to Top</a></li> <li class="nav-item nav-hidden"><p class="nav-link"></p></li> <li class="nav-item nav-hidden"><a class="nav-link" href="../net_1">Previous Part</a></li> <li class="nav-item nav-hidden"><a class="nav-link" href="">Next Part</a></li> <li class="nav-item nav-hidden"><p class="nav-link"></p></li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Topics </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/scale-ops/index">Introduction</a> <a class="dropdown-item " href="/scale-ops/net_1">Networking Part 1</a> <a class="dropdown-item " href="/scale-ops/net_2">Networking Part 2</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>The Impact of GPU Networking (Part 2)</h1> <p>Part 2 builds on earlier experiments by examining how distributing 4 GPUs across 1, 2, and 4 nodes impacts transformer model training, with a focus on network topology and NIC sharing.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#particularities-of-distributed-training-using-4-gpus-on-1-2-and-4-nodes">Particularities of Distributed Training using 4 GPUs on 1, 2, and 4 Nodes</a> </div> <div> <a href="#gpt-2-large-experiments">GPT-2 Large Experiments</a> </div> <div> <a href="#the-basics">The basics</a> </div> <div> <a href="#analysis">Analysis</a> </div> <div> <a href="#key-takeaways-tl-dr">Key Takeaways (TL;DR)</a> </div> <div> <a href="#additional-information">Additional Information</a> </div> </nav> </d-contents> <p><a href="../net_1">In Part 1</a>, I carried out a series of experiments to highlight the significance of GPU interconnects on the training performance of transformer-based models using Distributed Data Parallelism (DDP). These experiments demonstrated that even for relatively modest models, such as GPT-2 Large (700M parameters), the efficiency of the interconnect between GPUs or GPU nodes plays an important role in overall training time, even when data parallelism is used.</p> <p>Building upon these insights, in Part 2 I delve a bit deeper into the impact of networking when more GPU nodes are used in distributed training. I conduct a series of experiments using 4 GPUs distributed across 1, 2, and 4 nodes to illustrate how network topology and interconnects between nodes affect the training performance of transformer models.</p> <h2 id="particularities-of-distributed-training-using-4-gpus-on-1-2-and-4-nodes">Particularities of Distributed Training using 4 GPUs on 1, 2, and 4 Nodes</h2> <p>To begin, I ran a series of experiments using 4 GPUs distributed across 1, 2, and 4 nodes, and as in the Part 1, I used the <a href="https://www.baskerville.ac.uk/" rel="external nofollow noopener" target="_blank">Baskerville</a> HPC system (see Appendix A for system specifications). The experiments were conducted using the same GPT-2 Large model and training script as in Part 1.</p> <h3 id="gpt-2-large-experiments">GPT-2 Large Experiments</h3> <table> <thead> <tr> <th>Experiment</th> <th>Model</th> <th>Parallelism</th> <th>Nodes</th> <th>GPUs per Node</th> <th>Epoch 2 time (s)</th> </tr> </thead> <tbody> <tr> <td>e-15-7-5</td> <td>gpt2-large</td> <td>DDP</td> <td>1</td> <td>4</td> <td>135.42</td> </tr> <tr> <td>e-15-8-5</td> <td>gpt2-large</td> <td>DDP</td> <td>2</td> <td>2</td> <td>895.85</td> </tr> <tr> <td>e-15-9-4</td> <td>gpt2-large</td> <td>DDP</td> <td>4</td> <td>1</td> <td>251.68</td> </tr> </tbody> </table> <p>These results not only contradict H1 and H2 from Part 1—since the training times between <code class="language-plaintext highlighter-rouge">e-15-7-5</code> and <code class="language-plaintext highlighter-rouge">e-15-9-4</code> are almost double—but also show that intermediate configurations, such as <code class="language-plaintext highlighter-rouge">e-15-8-5</code> (2-node, 2-GPU-per-node), exhibit significantly slower training times compared to both <code class="language-plaintext highlighter-rouge">e-15-7-5</code> and <code class="language-plaintext highlighter-rouge">e-15-9-4</code>.</p> <p>To ensure that these results are not anomalies, I reused the same Python <a href="../Scripts/1_networking/bench_allreduce.py">script</a> from Part 1 to benchmark the performance of the all-reduce operation in a distributed PyTorch environment. As before, the script uses 700M parameters (GPT-2 Large) in BF16 precision (1.32 GB) and measures the transfer speed of the all-reduce operation between GPUs. The results are as follows:</p> <table> <thead> <tr> <th>Experiment</th> <th>Nodes</th> <th>GPUs per Node</th> <th>Measured Transfer speed (GB/s)</th> <th>Time (s)</th> </tr> </thead> <tbody> <tr> <td>e-16-4-1</td> <td>1</td> <td>4</td> <td>875.1</td> <td>0.009</td> </tr> <tr> <td>e-16-4-4</td> <td>2</td> <td>2</td> <td>25.6</td> <td>0.309</td> </tr> <tr> <td>e-16-4-3</td> <td>1</td> <td>4</td> <td>50.0</td> <td>0.158</td> </tr> </tbody> </table> <p><em>Note: The total data transferred per experiment in Table 2 is around 7.9 GB—that is, 1.32 GB transferred back and forth three times (N - 1).</em></p> <hr> <p><strong>Observation 1:</strong> The results from the all-reduce operation are consistent with the training times of the GPT-2 Large model and show that 2-node, 2-GPU-per-node configuration has a much slower transfer speed compared to the 1-node, 4-GPU-per-node or 4-node, 1-GPU-per-node configurations.</p> <hr> <p>This is somewhat counterintuitive, as one might expect that the transfer speed between 2 nodes with 2 GPUs per node would be faster than that between 4 nodes with 1 GPU per node, given that intra-node communication is clearly faster than inter-node communication. Thus, having fewer nodes should be more efficient. However, the results show that this is not the case.</p> <p>Let’s try to understand why this is happening by starting with some communication basics for distributed training.</p> <h2 id="the-basics">The basics</h2> <p>In DDP, each GPU processes a portion of the data and computes gradients independently. After each iteration, these gradients need to be synchronized (averaged) across all GPUs. This synchronization requires communication between GPUs, and the efficiency of this communication directly affects overall training time.</p> <p>To synchronize gradients, PyTorch uses the all-reduce operation—a collective communication operation that reduces the input tensor across all GPUs and returns the result to each GPU. Since I am using NVIDIA GPUs, the all-reduce operation is implemented with the NCCL library, which is optimized for NVIDIA GPUs and, when possible, uses the NVLink interconnect for communication between GPUs on the same node. The NCCL library also automatically detects the hardware topology and optimizes communication accordingly.</p> <p>There are several topologies that NCCL can employ, such as ring, tree, double binary tree, multi-node, and hybrid. The choice of topology depends on the hardware configuration and the number of GPUs used during training, and it is automatically selected by NCCL.</p> <h3 id="ring-topology">Ring topology</h3> <p>The ring topology is the most common topology used by NCCL. In this topology, GPUs are connected in a ring, with each GPU connected to two others. The ring topology is typically used when all GPUs are connected with fast interconnects, such as NVLink.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>       Rank 0
      /     \
 Rank 1       Rank 2
      \     /
       Rank 3
</code></pre></div></div> <p>The ring topology in the NCCL all-reduce operation is executed in two phases.</p> <p>In the first phase, each GPU splits its data into equal-sized chunks and initiates a reduce-scatter operation. Each rank sends one of its chunks to its neighbor while simultaneously receiving the corresponding chunk from the previous rank, adding the received values to its own. This exchange is performed over P–1 steps (with P being the total number of GPUs), ensuring that each chunk is progressively reduced across all nodes.</p> <p>In the second phase, an all-gather operation circulates these reduced chunks around the ring so that every rank eventually collects all chunks and obtains the complete, fully reduced result.</p> <h3 id="tree-topology">Tree topology</h3> <p>The tree topology is typically used when GPUs are not connected with NVLink and is more complex than the ring topology. In this topology, GPUs are connected in a tree structure, with each GPU connected to one or more others. The tree topology is typically used when GPUs are connected via InfiniBand.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    Rank 0
   /     \
Rank 1   Rank 2
           |
         Rank 3
</code></pre></div></div> <h3 id="double-binary-tree">Double binary tree</h3> <p>This enhanced topology uses two distinct binary trees — one for scattering (broadcast) and one for gathering (reduction) — to optimize bandwidth utilization.</p> <p>In this setup, one tree manages the reduction phase by aggregating data from all ranks into a single root (the “up” tree), while the other handles the broadcasting phase by distributing the aggregated result back to all ranks (the “down” tree).</p> <p>The reduction tree is identified by its final node having no children, as it collects data from every node, whereas the broadcast tree starts at a top node without a parent, initiating the dissemination process.</p> <h3 id="hybrid">Hybrid</h3> <p>Hybrid topologies allow NCCL to integrate multiple network structures — such as a ring within each node and a tree across nodes—tailoring the communication pattern to the underlying physical network. NCCL automatically detects and configures the best topology based on available hardware, though users can sometimes influence the selection through environment variables.</p> <p>For example, within a node, GPUs might be connected via NVLink forming a ring, while nodes are interconnected via Infiniband or Ethernet forming a tree. This approach employs a two-level design, first optimizing intra-node communication, then inter-node connectivity, which generally exhibits lower latency within nodes and higher latency between nodes.</p> <h2 id="analysis">Analysis</h2> <p>Let’s see which topology NCCL has detected for the <code class="language-plaintext highlighter-rouge">e-15-7-5</code>, <code class="language-plaintext highlighter-rouge">e-15-8-5</code>, and <code class="language-plaintext highlighter-rouge">e-15-9-4</code> experiments by adding <code class="language-plaintext highlighter-rouge">export NCCL_DEBUG=INFO</code> to the job submission script.</p> <p>The NCCL logs indicate that in every example, NCCL uses a combination of ring and tree topologies—a hybrid approach in which the best communication strategy is dynamically selected.</p> <p>The logs for the tree topology are given in the following format:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Trees P1/P2/P3-&gt;R-&gt;C
</code></pre></div></div> <p>where P is the parent(s) ranks (-1 if none), R is the current rank, and C is the child’s rank (-1 if none).</p> <p>In this context, parent in a reduction operation collects data from the child ranks, and in a broadcasting operation sends data to the child ranks, whereas the child does the opposite.</p> <p>Let’s look into the topologies of the experiments in more detail.</p> <h3 id="1-node-4-gpus">1 Node, 4 GPUs</h3> <p>In this case, all GPUs are on the same node which is NVLink connected. In this particular case, NCCL establishes 24 parallel communication “pipelines” (channels) to communicate between the GPUs. The high number of channels allows for a high level of parallelism, to make the most of the available NVLink bandwidth.</p> <p>Each channel sets up its own independent communication context, i.e., each one builds its own ring topology for point-to-point communication and, when applicable, its own tree structure for certain collective operations (like reductions). This design allows NCCL to overlap communication and computation across multiple channels, maximizing throughput and efficiency on multi-GPU systems.</p> <p>In this case, NCCL uses 24 ring and tree topologies, one for each channel, even though some of them are identical or may not be used.</p> <p>Rings:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>NCCL INFO Channel 00/24 :    0   1   2   3
NCCL INFO Channel 01/24 :    0   1   3   2
NCCL INFO Channel 02/24 :    0   2   3   1
...
NCCL INFO Channel 06/24 :    0   1   2   3
NCCL INFO Channel 07/24 :    0   1   3   2
...
NCCL INFO Channel 23/24 :    0   3   2   1
</code></pre></div></div> <p>Trees:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>NCCL INFO Trees [0] 1/-1/-1-&gt;0-&gt;-1 [1] 1/-1/-1-&gt;0-&gt;-1 [2] 1/-1/-1-&gt;0-&gt;-1 [3] 1/-1/-1-&gt;0-&gt;-1 [4] 2/-1/-1-&gt;0-&gt;-1 [5] 2/-1/-1-&gt;0-&gt;-1 ... [23] 3/-1/-1-&gt;0-&gt;1

NCCL INFO Trees [0] 2/-1/-1-&gt;1-&gt;0 [1] 2/-1/-1-&gt;1-&gt;0 [2] 2/-1/-1-&gt;1-&gt;0 [3] 2/-1/-1-&gt;1-&gt;0 [4] 3/-1/-1-&gt;1-&gt;2 [5] 3/-1/-1-&gt;1-&gt;2 ... [23] 0/-1/-1-&gt;1-&gt;-1

NCCL INFO Trees [0] 3/-1/-1-&gt;2-&gt;1 [1] 3/-1/-1-&gt;2-&gt;1 [2] 3/-1/-1-&gt;2-&gt;1 [3] 3/-1/-1-&gt;2-&gt;1 [4] 1/-1/-1-&gt;2-&gt;0 [5] 1/-1/-1-&gt;2-&gt;0 ... [23] -1/-1/-1-&gt;2-&gt;3

NCCL INFO Trees [0] -1/-1/-1-&gt;3-&gt;2 [1] -1/-1/-1-&gt;3-&gt;2 [2] -1/-1/-1-&gt;3-&gt;2 [3] -1/-1/-1-&gt;3-&gt;2 [4] -1/-1/-1-&gt;3-&gt;1 [5] -1/-1/-1-&gt;3-&gt;1 ... [23] 2/-1/-1-&gt;3-&gt;0
</code></pre></div></div> <h3 id="2-nodes-2-gpus-per-node">2 nodes, 2 GPUs per node</h3> <p>It is important to note that Rank 0 and Rank 1 are on the same node, and Rank 2 and Rank 3 are on the other node. In this case only 2 channels are established, and as in the previous case, NCCL uses a combination of ring and tree topologies in each channel.</p> <p>Rings:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>NCCL INFO Channel 00/02 :    0   1   2   3
NCCL INFO Channel 01/02 :    0   1   2   3
</code></pre></div></div> <p>Double binary trees:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>NCCL INFO Trees [0] -1/-1/-1-&gt;3-&gt;2 [1] -1/-1/-1-&gt;3-&gt;2
NCCL INFO Trees [0] 1/2/-1-&gt;0-&gt;-1 [1] 1/-1/-1-&gt;0-&gt;2
NCCL INFO Trees [0] 3/-1/-1-&gt;2-&gt;0 [1] 3/0/-1-&gt;2-&gt;-1
NCCL INFO Trees [0] -1/-1/-1-&gt;1-&gt;0 [1] -1/-1/-1-&gt;1-&gt;0

Tree 0: Reduction tree

           rank 0
          /      \
     rank 1       rank 2
                   |
                  rank 3

Tree 1: Broadcast tree

            rank 2
          /      \
    rank 3        rank 0
                    |
                  rank 1
</code></pre></div></div> <h3 id="4-nodes-1-gpu-per-node">4 nodes, 1 GPU per node</h3> <p>It is important to note that all ranks are on different nodes.</p> <p>Rings:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>NCCL INFO Channel 00/02 :    0   1   2   3
NCCL INFO Channel 01/02 :    0   1   2   3
</code></pre></div></div> <p>Double binary tree:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>NCCL INFO Trees [0] -1/-1/-1-&gt;3-&gt;2 [1] 1/-1/-1-&gt;3-&gt;-1
NCCL INFO Trees [0] -1/-1/-1-&gt;1-&gt;2 [1] 2/0/-1-&gt;1-&gt;3
NCCL INFO Trees [0] 1/3/-1-&gt;2-&gt;0 [1] -1/-1/-1-&gt;2-&gt;1
NCCL INFO Trees [0] 2/-1/-1-&gt;0-&gt;-1 [1] -1/-1/-1-&gt;0-&gt;1

Tree 0: Reduction tree

           Rank 0
             |
           Rank 2
          /      \
    Rank 1        Rank 3

Tree 1: Broadcast tree

           Rank 3
             |
           Rank 1
          /      \
    Rank 0        Rank 2
</code></pre></div></div> <h3 id="ring-or-tree">Ring or Tree?</h3> <p>Since the NCCL logs show that a combination of ring and tree topologies is being used in each of the experiments, it is difficult to say which topology is being used more or if the ring or tree topology is the bottleneck in the experiments. However, it is possible to set the topology manually using the <code class="language-plaintext highlighter-rouge">NCCL_ALGO</code> environment variable by setting it to <code class="language-plaintext highlighter-rouge">RING</code> or <code class="language-plaintext highlighter-rouge">TREE</code> to force the use of the ring or tree topology respectively and compare the results.</p> <table> <thead> <tr> <th>Experiment</th> <th>Nodes</th> <th>GPUs per Node</th> <th>Topology</th> <th>Measured Transfer speed (GB/s)</th> <th>Time (s)</th> </tr> </thead> <tbody> <tr> <td>e-16-5-1-r</td> <td>1</td> <td>4</td> <td>Ring</td> <td>922.4</td> <td>0.009</td> </tr> <tr> <td>e-16-5-1-t</td> <td>1</td> <td>4</td> <td>Tree</td> <td>606.6</td> <td>0.013</td> </tr> <tr> <td>e-16-5-2-r</td> <td>2</td> <td>2</td> <td>Ring</td> <td>13.5</td> <td>0.584</td> </tr> <tr> <td>e-16-5-2-t</td> <td>2</td> <td>2</td> <td>Tree</td> <td>25.6</td> <td>0.309</td> </tr> <tr> <td>e-16-5-3-r</td> <td>1</td> <td>4</td> <td>Ring</td> <td>53.9</td> <td>0.158</td> </tr> <tr> <td>e-16-5-3-t</td> <td>1</td> <td>4</td> <td>Tree</td> <td>45.0</td> <td>0.158</td> </tr> </tbody> </table> <hr> <p><strong>Observation 2:</strong> The results show that the ring topology is faster than the tree topology for the 1-node, 4-GPU-per-node and 4-node, 1-GPU-per-node configurations. However, the ring topology is slower than the tree topology for the 2-node, 2-GPU-per-node configuration.</p> <hr> <h3 id="why-is-the-2node-2gpupernode-configuration-slower">Why Is the 2‐Node, 2‐GPU‐per‐Node Configuration Slower?</h3> <p>Intuitively, the 2-node, 2-GPU-per-node configuration should be faster with a tree topology than the ring topology using 4 nodes with 1 GPU each, as the former has fewer nodes and relies less on inter-node communication. However, experiments consistently show the opposite outcome for both the GPT-2 Large model and a standalone PyTorch AllReduce operation.</p> <p>The only explanation for this discrepancy is the hardware configuration of the Baskerville system, where the experiments were conducted. In this system, each node has one network interface card (NIC) for the InfiniBand HDR network, shared by all GPUs on that node. This means that when two GPUs are on the same node, they must share the single NIC, effectively reducing the per-GPU bandwidth for inter-node communication.</p> <p>This can be clearly illustrated by comparing the <code class="language-plaintext highlighter-rouge">e-16-5-2-r</code> (2 nodes, 2 GPUs each) and <code class="language-plaintext highlighter-rouge">e-16-5-3-r</code>(4 nodes, 1 GPU each) experiments. The measured transfer speed for the 2-node, 2-GPU-per-node configuration is significantly slower—roughly four times slower than the 4-node setup. This is because <code class="language-plaintext highlighter-rouge">e-16-5-3-r</code> benefits from parallel bi-directional communication between the nodes, effectively doubling the bandwidth, whereas the 2-node configuration must share the NIC, effectively halving the bandwidth.</p> <p>This is also why the tree topology is faster than the ring topology for the 2-node, 2-GPU-per-node configuration (<code class="language-plaintext highlighter-rouge">e-16-5-2-t</code> vs. <code class="language-plaintext highlighter-rouge">e-16-5-2-r</code>). In the tree topology, results are first aggregated on the node and then sent to the other node, reducing NIC contention and mitigating the bandwidth bottleneck.</p> <h2 id="key-takeaways-tldr">Key Takeaways (TL;DR)</h2> <p>Key takeaways from these experiments underscore the importance of GPU distribution, network interfaces, and communication topologies in multi-node training. In particular:</p> <ul> <li>Collocating all GPUs on a single node (and leveraging high-bandwidth NVLink) yield notably faster training than spreading GPUs across multiple nodes.</li> <li>While theory suggests that fewer nodes with more GPUs each might be preferable, real-world hardware constraints—like sharing a single NIC per node—can drastically reduce effective bandwidth.</li> <li>Forcing ring or tree topologies demonstrates that ring can excel when GPUs do not contend for inter-node communication, whereas tree may perform better when multiple GPUs share a single NIC.</li> <li>Ultimately, empirical tests in actual cluster environments, rather than relying solely on theoretical models, are essential to identify the best GPU distribution and communication strategy for large-scale transformer training.</li> </ul> <p>Thank you for reading this far! I warmly welcome any feedback or discussion, especially if you spot potential oversights in my reasoning or experiments.</p> <h1 id="additional-information">Additional Information</h1> <h2 id="appendix-a-baskerville-hpc-system-specifications">Appendix A: Baskerville HPC System Specifications</h2> <p>The experiments were conducted on the <a href="https://www.baskerville.ac.uk/" rel="external nofollow noopener" target="_blank">Baskerville</a> HPC system, which has the following specifications:</p> <h3 id="compute-nodes">Compute Nodes</h3> <p>There are 57 SD650-N V2 liquid-cooled compute trays with:</p> <ul> <li>2× Intel® Xeon® Platinum 8360Y CPUs, each with 36 cores at 2.4GHz (boost up to 3.5GHz)</li> <li>512GB RAM (16× 32GB DDR4)</li> <li>1TB NVMe M.2 device (used for the OS and available as /scratch-local)</li> <li>1× 25GbE NVIDIA® Mellanox® adapter (on-planar ConnectX-4 port)</li> <li>1× HDR (200Gbps) NVIDIA Mellanox Infiniband port (ConnectX-6 PCIe Gen4 adapter)</li> <li>NVIDIA HGX-100 GPU planar</li> <li>4× NVIDIA A100 GPUs</li> </ul> <p>The GPUs on 11 nodes have 80GB RAM, while those on the remaining 46 nodes have 40GB RAM. The GPUs are interconnected using NVIDIA NVLINK.</p> <h3 id="network">Network</h3> <p>Baskerville uses three networks:</p> <ul> <li>An isolated 1GbE management network (not user-accessible)</li> <li>A 25GbE high-speed Ethernet network built using NVIDIA Mellanox Spectrum®-2 switches running NVIDIA Cumulus® Linux</li> <li>An HDR fat-tree InfiniBand network built using NVIDIA Mellanox Quantum HDR switches (QM8790) and ConnectX-6 PCIe gen 4 adapters which provides a full 200Gb network connection</li> </ul> <h3 id="storage">Storage</h3> <p>The system is equipped with Lenovo DSS-G storage systems running IBM® Spectrum Scale™:</p> <ul> <li>1× DSS-G250 equipped with 418× 16TB HDD</li> <li>1× DSS-G204 equipped with 48× 7.68TB SSD</li> </ul> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/scale-ops/assets/bibliography/"></d-bibliography> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 . Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/scale-ops/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/scale-ops/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/scale-ops/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/scale-ops/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/scale-ops/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/scale-ops/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/scale-ops/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/scale-ops/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/scale-ops/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>